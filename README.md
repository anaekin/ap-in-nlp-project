# Measuring Affective Content in Dialogue Summarization

This repository is the official PyTorch and huggingface implementation of Measuring Affective Content in Dialogue Summarization.

Dialogue summaries are essential for capturing both factual and affective content in applications like customer service, therapy, and nursing care. Existing NLP methods often focus on binary sentiment polarity and fail to capture complex emotional nuances. To address this, we propose novel evaluation metrics—**RACS** and **AACS** —which measure emotional alignment and retention in summaries. We further introduce **ERES** , a unified metric, supported by sentence-level emotion classification using a fine-tuned BERT model and summaries generated by a fine-tuned BART model. Our results demonstrate the effectiveness of this approach in enhancing emotion-aware summarization.

The code for the finetuning BART-Large model on DialogSum dataset present in the `bart_dialogsum` folder. The code for finetuning BERT model for sentence level emotion classification is in the `bert_carer_go_emotions` folder. The `extract_emotions.py` can be used to extract emotion from BART summaries and dialogues. Our proposed evaluation metric can be found in `custom_metric.py` and uses test set of the DialogSum dataset for evaluation. You can also fine the `custom_metric_per_emotion.py` file to get the individual emotions score.

Our results for the emotion extraction are present in the `results` folder.
