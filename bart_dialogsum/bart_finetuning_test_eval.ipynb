{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load ROUGE metric\n",
    "metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_summaries(model, tokenizer, dataset, num_examples=None, max_source_length=512):\n",
    "    \"\"\"Generates summaries and calculates ROUGE scores.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    all_decoded_preds = []\n",
    "    all_decoded_labels = []\n",
    "\n",
    "\n",
    "    # Process all examples if num_examples is not specified, otherwise process the specified number\n",
    "    num_examples = len(dataset) if num_examples is None else num_examples\n",
    "\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        input_dialogue = dataset[i][\"dialogue\"]\n",
    "        input_ids = tokenizer.encode(\n",
    "            input_dialogue, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate summary with specified parameters\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": 90,\n",
    "            \"min_length\": 1,\n",
    "            \"num_beams\": 6,\n",
    "            \"no_repeat_ngram_size\": 2,\n",
    "            \"length_penalty\": 1.6,\n",
    "            # \"do_sample\": True,  # Enables sampling\n",
    "            # \"top_k\": 50,        # Use top-k sampling\n",
    "            # \"top_p\": 0.8,      # Use nucleus sampling\n",
    "            # \"temperature\": 0.7  # Adjust temperature to control randomness\n",
    "        }\n",
    "\n",
    "        with torch.no_grad(): # Add this to disable gradient calculations\n",
    "            generated_ids = model.generate(input_ids=input_ids, **gen_kwargs)\n",
    "\n",
    "\n",
    "        generated_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        reference_summary = dataset[i]['summary']\n",
    "\n",
    "        all_decoded_preds.append(generated_summary)\n",
    "        all_decoded_labels.append(reference_summary)\n",
    "\n",
    "        # print(f\"Input Dialogue:\\n{input_dialogue}...\") # Shorten input for display\n",
    "        print(f\"Dialog: {i + 1}\")\n",
    "        print(f\"Generated Summary:\\n{generated_summary}\")\n",
    "        print(f\"Reference Summary:\\n{reference_summary}\")\n",
    "        print(\"-\" * 50)\n",
    "        # print(\"Running evaluation...\")\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate ROUGE scores after generating all summaries\n",
    "    result = metric.compute(predictions=all_decoded_preds, references=all_decoded_labels, use_stemmer=True)\n",
    "    result = {key: value * 100 for key, value in result.items()}  # Convert to percentage\n",
    "\n",
    "    # Print ROUGE scores\n",
    "    print(\"ROUGE Scores:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"{key}: {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "model_checkpoint = \"./finetuned_bart_dialogsum/checkpoint-2925\"\n",
    "model_checkpoint = \"facebook/bart-large\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_checkpoint)\n",
    "generate_summaries(model, tokenizer, test_dataset, num_examples=3) #Pass dataset, num_examples to limit the processing to 10 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
